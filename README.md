# Multilingual RAG Chatbot for Channel Sales  
**Trying to build VIBE CODED Prod grade LLM System with Feedback-Driven Learning, Cost Control & Zero-Downtime Embedding Migration**

[Roadmap](docs/nexsteps.md`)

---

## 1. Project Overview

This project implements a **production-grade conversational AI system** for **Channel Sales and Operations**.  
The chatbot answers questions over internal documents (pricing, partner policies, sales playbooks, tickets, etc.), supports **multiple languages**, and continuously improves via **user feedback-driven fine-tuning**.

The system is designed to mirror a **real enterprise deployment**:

- Retrieval-Augmented Generation (RAG)  
- Multilingual support  
- Token-level cost tracking  
- Low-latency API serving  
- Embedding versioning and backfill  
- Feedback loop for model improvement  
- Cloud-deployable microservices  

---

## 2. High-Level Architecture

```
User (Any Language)
|
Language Detection
|
Translation (if needed)
|
Query Embedding
|
Vector Search (FAISS / Pinecone)
|
Top-K Chunks
|
LLM (GPT / Mistral)
|
Post-processing & Validation
|
Translate Back
|
Final Answer
|
Feedback Collection
|
Training Dataset
```

---

## 3. Core Capabilities

### 3.1 Retrieval-Augmented Generation (RAG)

- Documents are chunked and embedded  
- Queries retrieve top-k relevant chunks  
- LLM answers grounded on retrieved context  

---

### 3.2 Multilingual Chat

- Automatic language detection  
- Translate queries to English  
- Translate answers back to user language  

---

### 3.3 Cost & Latency Awareness

Tracked per request:

- Input tokens  
- Output tokens  
- Embedding calls  
- LLM calls  
- Latency per stage 
- Cache common embeddings and responses to reduce cost 

Optimized via:

- Query embedding caching  
- LLM response caching  
- Model routing (GPT-4 â†’ GPT-3.5)  

---

### 3.4 Continuous Learning

Captured per interaction:
- prompt
- retrieved_documents
- generated_answer
- user_rating

Used for:

- Supervised fine-tuning (SFT)  
- Preference learning (DPO)  

---

### 3.5 Zero-Downtime Embedding Migration

- Multiple embedding versions  
- Dual indexing  
- Gradual traffic shifting  
- Background backfilling  

---

### 3.6 Enterprise Reliability

Fallbacks:

- GPT-4 â†’ GPT-3.5  
- Vector search â†’ keyword search  
- LLM â†’ cached answer  

---

## 4. Technology Stack

### Backend

| Component | Technology |
|--------|-------------|
| API | FastAPI |
| Auth | JWT / API Key |
| Async | Uvicorn |
| Tracing | OpenTelemetry |

---

### LLM & NLP

| Purpose | Tool |
|------|------|
| LLM | GPT-4 / Mistral |
| Embeddings | OpenAI / BGE / E5 |
| Translation | OpenAI / MarianMT |
| Lang Detection | fastText |

---

### Storage

| Data | Technology |
|------|-----------|
| Vectors | FAISS / Pinecone |
| Metadata | PostgreSQL |
| Feedback | PostgreSQL |
| Cache | Redis |

---

### MLOps

| Area | Tool |
|------|------|
| Model Registry | MLflow |
| Training | Weights & Biases |
| Metrics | Prometheus |
| Dashboards | Grafana |
| CI/CD | GitHub Actions |
| Deployment | Docker, AWS ECS / GCP Cloud Run |

---

## 5. Repository Structure
```
rag-chatbot/
â”‚
â”œâ”€â”€ ingestion/
â”‚   â”œâ”€â”€ loaders.py
â”‚   â”œâ”€â”€ chunker.py
â”‚   â”œâ”€â”€ embedder.py
â”‚   â””â”€â”€ index_builder.py
â”‚
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â”œâ”€â”€ chat.py
â”‚   â”‚   â”œâ”€â”€ feedback.py
â”‚   â”‚   â””â”€â”€ metrics.py
â”‚   â””â”€â”€ middleware.py
â”‚
â”œâ”€â”€ retrieval/
â”‚   â”œâ”€â”€ vector_store.py
â”‚   â”œâ”€â”€ reranker.py
â”‚   â””â”€â”€ fallback.py
â”‚
â”œâ”€â”€ llm/
â”‚   â”œâ”€â”€ prompt_templates.py
â”‚   â”œâ”€â”€ generator.py
â”‚   â””â”€â”€ postprocessor.py
â”‚
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ dataset_builder.py
â”‚   â”œâ”€â”€ sft.py
â”‚   â””â”€â”€ dpo.py
â”‚
â”œâ”€â”€ monitoring/
â”‚   â”œâ”€â”€ metrics.py
â”‚   â””â”€â”€ dashboards/
â”‚
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ models.yaml
â”‚   â”œâ”€â”€ embedding_versions.yaml
â”‚   â””â”€â”€ cost_limits.yaml
â”‚
â””â”€â”€ docker-compose.yml
```

---

## 6. Query Flow

1. User sends query  
2. Language detected  
3. Translated to English  
4. Query embedding generated  
5. Vector search  
6. Prompt built  
7. LLM generates answer  
8. Post-processing  
9. Translate back  
10. Return answer  
11. Store logs & feedback  

---

## 7. Metrics

### System
- Latency per stage  
- Requests/sec  
- Cache hit rate  
- Vector DB query time

### Model
- Cost per query  
- Token usage  
- Win-rate from feedback  
- Hallucination rate  

---

## 8. Feedback Loop

Users rate responses:

- ğŸ‘ Useful  
- ğŸ‘ Incorrect  

Stored as:

(prompt, retrieved_docs, answer, rating)

Used for :
- Supervised fine-tuning
- Preference learning (DPO)
- Prompt refinement

---

## 9. Embedding Versioning

Each chunk stores:

chunk_id
text
embedding_v1
embedding_v2

Retrieval router supports:
- 100% v1
- Shadow traffic to v2
- Gradual cutover
- Traffic can be routed gradually from v1 â†’ v2.

---